{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0c244da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to install numba and numpy\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39168c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Agent:\n",
    "    def __init__(self, cultural_sway):\n",
    "        self.strategy = None  # 'T' or 'I'\n",
    "        self.payoff = 0.0\n",
    "        self.cultural_sway = cultural_sway\n",
    "        self.history = []  \n",
    "        self.payoff_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56c745fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def payoff_matrix(theta, strat1, strat2):\n",
    "    if strat1 == 'T' and strat2 == 'T':\n",
    "        p = 16 - theta\n",
    "    elif strat1 == 'I' and strat2 == 'I':\n",
    "        p = theta\n",
    "    else:\n",
    "        p = 4\n",
    "    return p, p  # symmetric\n",
    "\n",
    "@jit(nopython=True)\n",
    "def update_strategies_jit(strategies, payoffs, beta):\n",
    "    new_strategies = strategies.copy()\n",
    "    n = len(strategies)\n",
    "    for i in range(n):\n",
    "        j = np.random.randint(0, n)\n",
    "        while j == i:\n",
    "            j = np.random.randint(0, n)\n",
    "        prob = 1 / (1 + np.exp(-beta * (payoffs[j] - payoffs[i])))\n",
    "        if np.random.random() < prob:\n",
    "            new_strategies[i] = strategies[j]\n",
    "    return new_strategies\n",
    "\n",
    "def payoff_matrix_jit(theta, strat1, strat2):\n",
    "    if strat1 == 0 and strat2 == 0:\n",
    "        p = 16 - theta\n",
    "    elif strat1 == 1 and strat2 == 1:\n",
    "        p = theta\n",
    "    else:\n",
    "        p = 4\n",
    "    return p, p\n",
    "\n",
    "class Game:\n",
    "    def __init__(self, theta, beta, max_steps, population):\n",
    "        self.theta = theta\n",
    "        self.beta = beta\n",
    "        self.max_steps = max_steps\n",
    "        self.population = population\n",
    "        for agent in population:\n",
    "            agent.payoff = 0.0\n",
    "            agent.history = []\n",
    "            agent.payoff_history = []\n",
    "        self.strategies = np.array([0 if a.strategy == 'T' else 1 for a in population])\n",
    "        self.payoffs = np.zeros(len(population))\n",
    "        logging.info(f\"Initialized game for theta {theta}, beta {beta}, max_steps {max_steps}, population size {len(population)}\")\n",
    "\n",
    "    def update_payoff(self):\n",
    "        # Compute population fractions\n",
    "        frac_t = np.mean(self.strategies == 0)\n",
    "        frac_i = 1 - frac_t\n",
    "        \n",
    "        # Calculate average payoff for each agent\n",
    "        payoff_tt = payoff_matrix_jit(self.theta, 0, 0)[0]\n",
    "        payoff_ti = payoff_matrix_jit(self.theta, 0, 1)[0]\n",
    "        payoff_it = payoff_matrix_jit(self.theta, 1, 0)[0]\n",
    "        payoff_ii = payoff_matrix_jit(self.theta, 1, 1)[0]\n",
    "        \n",
    "        self.payoffs = np.where(self.strategies == 0, \n",
    "                                frac_t * payoff_tt + frac_i * payoff_ti,\n",
    "                                frac_t * payoff_it + frac_i * payoff_ii)\n",
    "        \n",
    "        # Update agent objects and record history\n",
    "        for i, agent in enumerate(self.population):\n",
    "            agent.payoff = self.payoffs[i]\n",
    "            agent.payoff_history.append(agent.payoff)\n",
    "\n",
    "    def update_strategies(self):\n",
    "        self.strategies = update_strategies_jit(self.strategies, self.payoffs, self.beta)\n",
    "        \n",
    "        # Update agent objects\n",
    "        for i, agent in enumerate(self.population):\n",
    "            agent.strategy = 'T' if self.strategies[i] == 0 else 'I'\n",
    "\n",
    "    def run(self):\n",
    "        logging.info(f\"Starting evolution for {self.max_steps} steps\")\n",
    "        for step in range(self.max_steps):\n",
    "            self.update_payoff()\n",
    "            self.update_strategies()\n",
    "            # Record history after each evolution step\n",
    "            for agent in self.population:\n",
    "                agent.history.append(agent.strategy)\n",
    "        logging.info(f\"Evolution completed for theta {self.theta}\")\n",
    "        # Return equilibrium: fractions T and I\n",
    "        t_count = sum(1 for a in self.population if a.strategy == 'T')\n",
    "        i_count = len(self.population) - t_count\n",
    "        frac_t = t_count / len(self.population)\n",
    "        frac_i = i_count / len(self.population)\n",
    "        fractions = {'T': frac_t, 'I': frac_i}\n",
    "        logging.info(f\"Final fractions: {fractions}\")\n",
    "        return fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03f186c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulation:\n",
    "    def __init__(self, N, f_cultural, theta_list, beta, max_steps, ensemble_size):\n",
    "        self.N = N\n",
    "        self.f_cultural = f_cultural\n",
    "        self.theta_list = theta_list\n",
    "        self.beta = beta\n",
    "        self.max_steps = max_steps\n",
    "        self.ensemble_size = ensemble_size\n",
    "        self.population = [Agent(random.random() < f_cultural) for _ in range(N)]\n",
    "        self.equilibria = {}  # theta: averaged fractions\n",
    "        logging.info(f\"Initialized simulation with N={N}, f_cultural={f_cultural}, theta_list={theta_list}, beta={beta}, max_steps={max_steps}, ensemble_size={ensemble_size}\")\n",
    "\n",
    "    def initialize_for_theta(self, theta):\n",
    "        if not self.equilibria:\n",
    "            for agent in self.population:\n",
    "                agent.strategy = random.choice(['T', 'I'])\n",
    "            logging.info(f\"Initialized strategies randomly for first theta {theta}\")\n",
    "        else:\n",
    "            prev_thetas = list(self.equilibria.keys())\n",
    "            closest_theta = min(prev_thetas, key=lambda t: abs(t - theta))\n",
    "            frac_dict = self.equilibria[closest_theta]\n",
    "            for agent in self.population:\n",
    "                if agent.cultural_sway:\n",
    "                    agent.strategy = 'T' if random.random() < frac_dict['T'] else 'I'\n",
    "                else:\n",
    "                    agent.strategy = random.choice(['T', 'I'])\n",
    "            logging.info(f\"Initialized strategies for theta {theta} based on closest previous theta {closest_theta} with T fraction {frac_dict['T']}\")\n",
    "\n",
    "    def average_fractions(self, frac_list):\n",
    "        if not frac_list:\n",
    "            return {}\n",
    "        keys = frac_list[0].keys()\n",
    "        avg = {}\n",
    "        for k in keys:\n",
    "            avg[k] = sum(f[k] for f in frac_list) / len(frac_list)\n",
    "        return avg\n",
    "\n",
    "    def run_simulation(self):\n",
    "        os.makedirs('results', exist_ok=True)\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        run_dir = f'results/run_{timestamp}'\n",
    "        os.makedirs(run_dir, exist_ok=True)\n",
    "        self.data = {}\n",
    "        logging.info(\"Starting full simulation run\")\n",
    "        for theta in self.theta_list:\n",
    "            logging.info(f\"Processing theta {theta}\")\n",
    "            self.initialize_for_theta(theta)\n",
    "            initial_strategies = [a.strategy for a in self.population]\n",
    "            ensemble_fractions = []\n",
    "            self.data[theta] = {}\n",
    "            for i in range(self.ensemble_size):\n",
    "                # Reset strategies to initial\n",
    "                for a, s in zip(self.population, initial_strategies):\n",
    "                    a.strategy = s\n",
    "                game = Game(theta, self.beta, self.max_steps, self.population)\n",
    "                game.strategies = np.array([0 if a.strategy == 'T' else 1 for a in self.population])\n",
    "                fractions = game.run()\n",
    "                ensemble_fractions.append(fractions)\n",
    "                # Collect strategies and payoffs\n",
    "                strategies = [agent.history for agent in self.population]\n",
    "                payoffs = [agent.payoff_history for agent in self.population]\n",
    "                self.data[theta][i] = {'strategies': strategies, 'payoffs': payoffs}\n",
    "            # Average fractions\n",
    "            averaged_fractions = self.average_fractions(ensemble_fractions)\n",
    "            self.equilibria[theta] = averaged_fractions\n",
    "            logging.info(f\"Completed theta {theta}, averaged final fractions: {averaged_fractions}\")\n",
    "        logging.info(f\"Simulation completed. Equilibria: {self.equilibria}\")\n",
    "        # Save data to pickle\n",
    "        with open(f'{run_dir}/simulation_data.pkl', 'wb') as f:\n",
    "            pickle.dump(self.data, f)\n",
    "        # Save metadata to YAML\n",
    "        metadata = {\n",
    "            'N': self.N,\n",
    "            'f_cultural': self.f_cultural,\n",
    "            'theta_list': self.theta_list,\n",
    "            'beta': self.beta,\n",
    "            'max_steps': self.max_steps,\n",
    "            'ensemble_size': self.ensemble_size,\n",
    "            'equilibria': [{'theta': theta, **self.equilibria[theta]} for theta in self.theta_list],\n",
    "            'timestamp': timestamp,\n",
    "            'run_dir': run_dir\n",
    "        }\n",
    "        with open(f'{run_dir}/simulation_metadata.yaml', 'w') as f:\n",
    "            yaml.dump(metadata, f)\n",
    "        return self.equilibria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665d3074",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "logging.info(\"Starting main simulation\")\n",
    "# Parameters\n",
    "N = 1000 # ensure this is even\n",
    "f_cultural = 3 / 4\n",
    "theta_list = [15, 1, 13, 1]\n",
    "beta = 0.01\n",
    "max_steps = 50\n",
    "ensemble_size = 100\n",
    "\n",
    "sim = Simulation(N, f_cultural, theta_list, beta, max_steps, ensemble_size)\n",
    "equilibria = sim.run_simulation()\n",
    "logging.info(\"Main simulation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830d3f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
